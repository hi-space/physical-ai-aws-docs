---
description: 'OpenVLA: An Open-Source Vision-Language-Action Model (2024)'
---

# OpenVLA

> _인터넷 규모의 비전-언어 데이터와 다양한 로봇 시연을 결합하여 사전 학습된 대규모 정책은 로봇에게 새로운 기술을 가르치는 방식을 바꿀 잠재력을 가지고 있습니다._

### 들어가며

로봇 조작 학습의 가장 큰 난제는 무엇일까요? 바로 **일반화(generalization)** 문제입니다. 현재 로봇 정책은 학습 데이터에서 본 상황에서는 잘 작동하지만, 새로운 물체, 다른 조명, 예상치 못한 방해 요소가 등장하면 실패하는 경우가 많습니다. 마치 시험 문제만 외운 학생처럼, 응용력이 부족한 것이죠.

하지만 흥미로운 사실이 있습니다. 컴퓨터 비전과 자연어 처리 분야에서는 CLIP, SigLIP, Llama2 같은 파운데이션 모델(Foundation Model)들이 놀라운 일반화 능력을 보여주고 있습니다. 이들은 인터넷에서 수집한 수십억 개의 이미지와 텍스트로 학습하여 본 적 없는 물체와 개념도 이해할 수 있습니다. 반면 가장 큰 로봇 데이터셋도 고작 100만 개 정도의 예제만 가지고 있죠.

이러한 불균형이 바로 **기회**입니다. 만약 인터넷 규모로 학습된 비전-언어 모델을 로봇 제어에 활용할 수 있다면 어떨까요? 오늘 소개할 논문 "OpenVLA"는 바로 이 아이디어를 실현합니다. 70억 개의 매개변수를 가진 오픈소스 비전-언어-행동(Vision-Language-Action, VLA) 모델로, 970,000개의 실제 로봇 시연으로 학습되어 여러 로봇을 즉시 제어할 수 있습니다.

이 글에서는 OpenVLA가 어떻게 작동하는지, 기존 접근법과 어떻게 다른지, 그리고 실제 성능은 어떠한지 살펴보겠습니다.

***

### 배경: 왜 비전-언어 모델이 로봇에 필요한가?

#### 기존 로봇 학습의 한계

전통적인 로봇 조작 학습 방법은 크게 두 가지 문제를 가지고 있습니다:

1. **데이터 부족**: 로봇 데이터를 수집하려면 실제 로봇을 움직여야 하므로 비용과 시간이 많이 듭니다. 가장 큰 데이터셋도 100만 개 정도의 예제에 불과합니다.
2. **일반화 부족**: 훈련 데이터에 포함된 물체와 상황에서만 잘 작동하고, 새로운 물체나 작업에는 대응하지 못합니다.

#### 비전-언어 모델(VLM)의 등장

최근 몇 년간 컴퓨터 비전과 자연어 처리가 통합된 VLM이 큰 발전을 이루었습니다. 이들은 다음과 같은 특징을 가집니다:

* **멀티모달 이해**: 이미지와 텍스트를 동시에 처리하여 "빨간 병을 집어라"와 같은 지시를 이해할 수 있습니다.
* **제로샷 일반화**: 학습하지 않은 물체도 인터넷 사전 학습 덕분에 인식할 수 있습니다.
* **스케일의 힘**: 수십억 개의 이미지-텍스트 쌍으로 학습하여 풍부한 시각적, 의미적 지식을 보유합니다.

#### 비전-언어-행동 모델(VLA)이란?

VLA는 VLM을 로봇 제어에 직접 활용하는 접근법입니다. 핵심 아이디어는 간단합니다:

1. 인터넷 규모로 사전 학습된 VLM을 가져옵니다.
2. 로봇 행동을 언어 모델의 "토큰"처럼 취급합니다.
3. 로봇 시연 데이터로 VLM을 미세 조정하여 행동을 예측하도록 합니다.

이전 연구인 RT-2가 이 방법으로 인상적인 결과를 보였지만, 두 가지 문제가 있었습니다:

* **폐쇄성**: 모델, 코드, 훈련 데이터가 공개되지 않음
* **적응성 부족**: 새로운 로봇이나 작업에 어떻게 적응시킬지 가이드가 없음

OpenVLA는 이 두 문제를 모두 해결한 **최초의 완전 오픈소스 범용 VLA**입니다.

***

### OpenVLA의 핵심 아키텍처

#### 전체 구조: 세 가지 핵심 구성 요소

OpenVLA의 아키텍처는 다음 세 부분으로 구성됩니다:

&#x20;_그림 1: OpenVLA 모델 아키텍처. 이미지 관찰과 언어 지시를 입력받아 7차원 로봇 제어 행동을 예측합니다._

**1. 비전 인코더: 두 개의 눈으로 보는 세상**

OpenVLA는 독특하게 **두 개의 비전 인코더를 융합**합니다:

* **SigLIP**: 고수준 의미 정보 포착 (예: "이것은 사과다")
* **DINOv2**: 저수준 공간 정보 포착 (예: "사과가 여기 있고, 저기서 3cm 떨어져 있다")

왜 두 개를 쓸까요? 로봇 제어는 "무엇"을 잡을지(의미)와 "어디서" 잡을지(공간)를 모두 알아야 하기 때문입니다. 이전 VLA들이 하나의 비전 인코더만 사용한 것과 차별화되는 지점입니다.

**2. 프로젝터: 비전과 언어를 연결하는 다리**

비전 인코더의 출력(이미지 패치 임베딩)을 언어 모델이 이해할 수 있는 공간으로 매핑하는 작은 2층 MLP입니다. 마치 통역사처럼 이미지 "언어"를 텍스트 "언어"로 번역하는 역할이죠.

**3. 언어 모델 백본: Llama 2 7B**

핵심은 70억 개의 매개변수를 가진 Llama 2 언어 모델입니다. 하지만 일반적인 텍스트를 생성하는 대신, **로봇 행동을 토큰으로 예측**하도록 미세 조정되었습니다.

#### 행동을 어떻게 토큰으로 만들까?

이것이 OpenVLA의 핵심 트릭입니다. 로봇 행동은 연속적인 숫자입니다 (예: x축으로 0.05m 이동, z축으로 2도 회전). 하지만 언어 모델은 이산적인 토큰만 다룰 수 있죠.

**해결책**: 각 행동 차원을 \*\*256개의 빈(bin)\*\*으로 이산화합니다.

```
연속 행동: Δx = 0.0523m
↓ 이산화
토큰: bin_142
```

* 훈련 데이터의 1번째와 99번째 백분위수 사이를 256개로 균등 분할
* 7차원 행동 = 7개의 토큰으로 표현
* 언어 모델의 다음 토큰 예측 목표로 학습

#### 훈련 절차: 두 단계 학습

**1단계: VLM 사전 학습 (Prismatic-7B)**

* SigLIP + DINOv2 + Llama 2를 결합
* 약 100만 개의 이미지-텍스트 쌍으로 학습 (LLaVA 1.5 데이터)
* 비전과 언어를 정렬(align)

**2단계: VLA 미세 조정 (OpenVLA)**

* Open X-Embodiment 데이터셋의 970,000개 로봇 궤적으로 미세 조정
* 70개 이상의 로봇 데이터셋을 큐레이션
* 27 에폭 동안 훈련 (일반 VLM은 1-2 에폭만 훈련하는 것과 대조적)

중요한 설계 결정들:

* **비전 인코더도 미세 조정**: VLM 학습에서는 보통 동결하지만, VLA는 미세한 공간 정보가 중요하므로 미세 조정
* **224x224 해상도**: 384x384보다 3배 빠르지만 성능 차이 없음
* **고정 학습률 2e-5**: 학습률 워밍업 불필요
* **95% 이상 토큰 정확도까지 훈련**: 일반 LLM보다 훨씬 많이 반복

***

### 훈련 데이터: 다양성이 핵심

OpenVLA는 **Open X-Embodiment** 데이터셋을 기반으로 970,000개의 로봇 시연을 큐레이션했습니다.

#### 데이터 큐레이션 전략

**목표**:

1. 다양한 로봇 구현체, 장면, 작업 포함
2. 일관된 입력/출력 공간
3. 균형 잡힌 데이터 혼합

**필터링 기준**:

* 최소 하나의 3인칭 카메라 필요
* 단일 팔 끝단 작동기 제어만 포함
* 조작(manipulation) 작업에 집중

**데이터 가중치**:

* Octo의 휴리스틱 가중치 활용
* 다양성이 낮은 데이터셋은 하향 조정
* 작업/장면 다양성이 높은 데이터셋은 상향 조정

**특별 실험**:

* DROID 데이터셋 추가 시도 (10% 가중치)
* 행동 토큰 정확도가 낮아 훈련 마지막 1/3에서 제거
* 더 큰 모델이나 가중치 조정이 필요할 것으로 판단

***

### 실험 결과: 최첨단 성능 입증

OpenVLA의 성능을 세 가지 질문을 통해 평가했습니다:

#### 1. 기본 성능: 여러 로봇에서 즉시 작동하는가?

**평가 설정**:

* **WidowX 로봇** (BridgeData V2): 17개 작업, 각 10회 시도
* **Google 로봇**: 12개 작업, 각 5회 시도
* 다양한 일반화 축 평가: 시각적, 동작, 물리적, 의미적

**비교 대상**:

* RT-1-X (35M 파라미터): OpenX로 학습한 트랜스포머
* Octo (93M 파라미터): 오픈소스 최고 범용 정책
* RT-2-X (55B 파라미터): 폐쇄형 최첨단 VLA

**결과**:

&#x20;_그림 2: BridgeData V2 WidowX 로봇 평가 결과. OpenVLA는 모든 범주에서 높은 성능을 보이며 RT-2-X를 능가합니다._

| 모델               | BridgeData V2 평균 성공률 | Google 로봇 평균 성공률 |
| ---------------- | -------------------- | ---------------- |
| RT-1-X           | 26.3%                | 14.3%            |
| Octo             | 29.0%                | 32.0%            |
| RT-2-X           | 70.6%                | 85.0%            |
| **OpenVLA (7B)** | **87.0%**            | **88.0%**        |

**주요 발견**:

* OpenVLA는 **RT-2-X보다 16.5% 높은 성공률** (WidowX)
* **7배 적은 파라미터**로 더 나은 성능 (7B vs 55B)
* Google 로봇에서도 RT-2-X와 동등하거나 우수
* RT-1-X와 Octo는 방해 물체가 있을 때 크게 어려움을 겪음

**정성적 관찰**:

* OpenVLA와 RT-2-X는 방해 물체가 있어도 올바른 물체에 접근
* 끝단 작동기의 방향을 물체에 맞게 적절히 조정
* 실수(예: 불안정한 파지)에서 회복하는 능력

#### 2. 적응성: 새로운 로봇에 빠르게 미세 조정할 수 있는가?

**평가 설정**:

* **Franka-Tabletop**: 고정된 테이블 장착형 Franka 로봇 (5Hz)
* **Franka-DROID**: 이동식 스탠딩 데스크의 Franka 로봇 (15Hz)
* 작업당 10-150개의 시연으로 미세 조정

**비교 대상**:

* Diffusion Policy: 최첨단 모방 학습 (처음부터 학습)
* Octo (fine-tuned): 미세 조정된 범용 정책
* OpenVLA (fine-tuned): 미세 조정된 OpenVLA

**결과**:

&#x20;_그림 3: 새로운 로봇 설정에 대한 적응 성능. OpenVLA는 다양한 작업에서 균형 잡힌 성능을 보입니다._

**주요 발견**:

1. **단일 지시 작업**: Diffusion Policy가 경쟁력 있음
   * "당근을 그릇에 넣기": Diffusion Policy 66.7%, OpenVLA 60.0%
   * 좁고 정교한 작업에서는 행동 청킹이 유리
2. **다중 지시 작업**: OpenVLA가 명확히 우수
   * "방해 물체와 함께 다양한 지시": OpenVLA 70.0%, Diffusion Policy 35.0%
   * 언어 기반이 중요한 작업에서 사전 학습의 이점
3. **전반적 성능**: OpenVLA가 최고
   * **모든 작업에서 50% 이상 성공률** 달성
   * 범용성과 안정성의 균형이 뛰어남

**OpenVLA (scratch) 실험**:

* OpenX 사전 학습 없이 Prismatic만 미세 조정
* 성능이 크게 떨어짐
* 대규모 로봇 사전 학습의 중요성 입증

#### 3. 효율성: 저렴한 GPU로도 가능한가?

**매개변수 효율적 미세 조정 (LoRA)**

**문제**: 전체 미세 조정은 8개 A100 GPU를 5-15시간 필요

**해결책**: 다양한 미세 조정 전략 비교

| 전략              | 성공률       | 훈련 파라미터 (백만) | GPU 메모리    |
| --------------- | --------- | ------------ | ---------- |
| 전체 미세 조정        | 69.7%     | 7,188.1      | 163.3GB\*  |
| 마지막 레이어만        | 30.3%     | 465.1        | 51.4GB     |
| 동결된 비전          | 47.0%     | 6,760.4      | 156.2GB\*  |
| 샌드위치            | 62.1%     | 914.2        | 64.0GB     |
| **LoRA (r=32)** | **68.2%** | **97.6**     | **59.7GB** |

**핵심 발견**:

* LoRA는 **1.4%의 파라미터만 미세 조정**하면서 전체 미세 조정과 동등한 성능
* **단일 A100 GPU에서 10-15시간**에 미세 조정 가능 (8배 계산 감소)
* 비전 인코더 미세 조정이 중요 (동결하면 성능 크게 하락)

**메모리 효율적 추론 (양자화)**

**문제**: OpenVLA는 bfloat16으로 16.8GB GPU 메모리 필요

**해결책**: 8비트 및 4비트 양자화 테스트

| 정밀도      | BridgeData V2 성공률 | GPU 메모리   |
| -------- | ----------------- | --------- |
| bfloat16 | 71.3%             | 16.8GB    |
| int8     | 58.1%             | 10.2GB    |
| **int4** | **71.9%**         | **7.0GB** |

**핵심 발견**:

* **4비트 양자화는 성능 손실 없음**
* GPU 메모리 **60% 감소** (16.8GB → 7.0GB)
* 8GB GPU (예: RTX 3070)에서도 실행 가능
* 추론 속도도 유지 (3Hz)

***

### 왜 OpenVLA가 중요한가?

#### 1. 연구자에게

**VLA 연구의 새로운 출발점**:

* 최초의 완전 오픈소스 VLA (모델, 데이터, 코드)
* 다양한 설계 결정에 대한 통찰 제공
* 더 나은 VLA 개발을 위한 기준선

**탐구할 질문들**:

* 더 큰 VLM (예: 70B)을 사용하면 어떨까?
* 로봇 데이터와 인터넷 데이터를 공동 학습하면?
* 최적의 비전 백본 조합은?

#### 2. 실무자에게

**즉시 사용 가능한 솔루션**:

* 여러 로봇을 기본적으로 제어
* 새로운 작업에 빠르게 적응 (10-150 시연)
* 소비자급 GPU에서 실행 가능

**적용 분야**:

* 다양한 물체를 다루는 조작 작업
* 언어 지시가 중요한 작업
* 제한된 GPU 자원으로 배포

**주의 사항**:

* 단일 이미지 입력만 지원 (현재)
* 5Hz 이하의 제어 빈도 권장
* 매우 정교한 양손 작업에는 Diffusion Policy가 더 나을 수 있음

#### 3. 로봇 커뮤니티에게

**접근성 향상**:

* 오픈소스로 누구나 실험 가능
* HuggingFace에서 쉽게 다운로드 및 미세 조정
* 상세한 문서와 노트북 제공

**생태계 구축**:

* 다른 연구자들이 기여하고 개선할 수 있는 플랫폼
* VLA 표준화의 시작점
* 언어 모델 생태계(예: Llama, BERT)처럼 성장 가능

***

### 한계점과 미래 방향

#### 현재 한계

**1. 입력 제약**:

* 단일 이미지만 지원
* 고유 감각 정보 (proprioception) 미사용
* 관찰 이력 미포함

**해결 방향**: 인터리브된 이미지-텍스트로 사전 학습된 VLM 활용

**2. 추론 속도**:

* 현재 3-6Hz (GPU에 따라)
* 고주파수 제어 (예: ALOHA 50Hz) 불가능

**해결 방향**:

* 행동 청킹 (Diffusion Policy처럼)
* 투기적 디코딩 (Speculative Decoding)
* TensorRT-LLM 같은 최적화 프레임워크

**3. 신뢰성**:

* 최고 성공률 90% 미만
* 산업 수준 (>95%)에는 미달

**해결 방향**:

* 더 많은 데이터와 더 큰 모델
* 인터넷 데이터와 로봇 데이터 공동 학습
* 강화 학습과의 결합

#### 흥미로운 미래 방향

**1. 멀티모달 확장**:

* 텍스트 + 이미지 + 오디오를 함께 처리
* 예: "냉장고 소리가 나면 문을 닫아"

**2. 하이브리드 아키텍처**:

* 일부 층은 Transformer (전역 주의)
* 일부 층은 Mamba (효율성)
* 최고의 조합 탐색

**3. 긴 컨텍스트**:

* 현재: 단일 이미지
* 미래: 수백 프레임의 비디오로 시간적 추론

**4. 강화 학습 통합**:

* 시연 학습 (Imitation) + 탐색 학습 (RL)
* 자가 개선 능력

***

### 마치며

OpenVLA는 로봇 조작 학습의 새로운 장을 열었습니다. 인터넷 규모의 비전-언어 사전 학습과 대규모 로봇 데이터를 결합하여, 이전에는 불가능했던 수준의 일반화와 적응 능력을 보여줍니다.

OpenVLA는 "Transformer의 GPT" 같은 역할을 VLA 분야에서 할 잠재력이 있습니다. 오픈소스로 공개되어 누구나 사용하고 개선할 수 있으며, 표준화된 출발점을 제공합니다. 이제 로봇 학습 연구자들은 처음부터 다시 시작할 필요 없이, OpenVLA를 기반으로 자신의 아이디어를 빠르게 실험할 수 있습니다.

로봇이 인터넷의 지식을 활용하여 현실 세계에서 작동하는 미래가 점점 가까워지고 있습니다. OpenVLA는 그 미래로 가는 중요한 이정표입니다.

***

### 참고 자료

* 📄 **논문**: [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246)
* 💻 **코드**: [https://github.com/openvla/openvla](https://openvla.github.io)
* 🤗 **모델**: HuggingFace에서 다운로드 가능
* 🎥 **데모**: [https://openvla.github.io](https://openvla.github.io)

